{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e36b0dea",
      "metadata": {
        "id": "e36b0dea"
      },
      "source": [
        "## Supervised Learning in the Perceptron Neuron\n",
        "\n",
        "### Team Members\n",
        "- **Juliana Ballin Lima** – Registration: 2315310011  \n",
        "- **Marcelo Heitor de Almeida Lira** – Registration: 2315310043  \n",
        "- **Lucas Maciel Gomes** – Registration: 2315310014  \n",
        "- **Ryan da Silva Marinho** – Registration: 2315310047  \n",
        "- **Vitória Gabrielle Kinshasa Silva de Almeida** – Registration: 2415280044"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQ22KTGBCWuC"
      },
      "id": "SQ22KTGBCWuC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "256c75ff",
      "metadata": {
        "id": "256c75ff"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Libraries required for the project.\n",
        "Mandatory: numpy, random, math, matplotlib.\n",
        "sklearn is allowed only for performance metrics.\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from prettytable import PrettyTable\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8518726",
      "metadata": {
        "id": "e8518726",
        "outputId": "91a51a56-c9f5-4d71-af2e-e7a87e201e42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Juliana Ballin Lima - 2315310011\n",
            "Marcelo Heitor de Almeida Lira - 2315310043\n",
            "Lucas Maciel Gomes - 2315310014\n",
            "Ryan da Silva Marinho - 2315310047\n",
            "Vitória Gabrielle Kinshasa Silva de Almeida - 2415280044\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Display team members in a structured way.\n",
        "This ensures clarity and meets evaluation criteria.\n",
        "\"\"\"\n",
        "\n",
        "team_members: dict[str, str] = {\n",
        "    \"Juliana Ballin Lima\": \"2315310011\",\n",
        "    \"Marcelo Heitor de Almeida Lira\": \"2315310043\",\n",
        "    \"Lucas Maciel Gomes\": \"2315310014\",\n",
        "    \"Ryan da Silva Marinho\": \"2315310047\",\n",
        "    \"Vitória Gabrielle Kinshasa Silva de Almeida\": \"2415280044\"\n",
        "}\n",
        "\n",
        "for name, reg in team_members.items():\n",
        "    print(f\"{name} - {reg}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88214852",
      "metadata": {
        "id": "88214852",
        "outputId": "a7e8c4b4-0cc1-4680-816a-4998f53c5fac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Team identifier: 3\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Calculate team identifier based on the sum of the last digits\n",
        "of registration numbers, applying modulo 4.\n",
        "\"\"\"\n",
        "\n",
        "# Last digits of registrations\n",
        "ids: list[int] = [1, 3, 4, 7, 4]\n",
        "team_sum: int = sum(ids)\n",
        "team_identifier: int = team_sum % 4\n",
        "\n",
        "print(\"Team identifier:\", team_identifier)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d079be14",
      "metadata": {
        "id": "d079be14"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Utility functions for the Perceptron algorithm.\n",
        "Includes activation and training procedure.\n",
        "\"\"\"\n",
        "\n",
        "def activation_function(u: float) -> int:\n",
        "    \"\"\"\n",
        "    Step activation function with threshold θ = 0.\n",
        "\n",
        "    Args:\n",
        "        u (float): Linear combination input\n",
        "\n",
        "    Returns:\n",
        "        int: 1 if u >= 0 else 0.\n",
        "    \"\"\"\n",
        "    return 1 if u >= 0 else 0\n",
        "\n",
        "\n",
        "def perceptron_train(X, y, eta=0.1, max_epochs=1000, initial_weights=None) -> tuple[np.ndarray, int, int, np.ndarray | None]:\n",
        "    \"\"\"\n",
        "    Perceptron training until convergence or max_epochs.\n",
        "\n",
        "    Args:\n",
        "        X (ndarray): input data with bias\n",
        "        y (ndarray): labels (0 or 1)\n",
        "        eta (float): learning rate\n",
        "        max_epochs (int): maximum iterations\n",
        "        initial_weights (ndarray): optional starting weights\n",
        "\n",
        "    Returns:\n",
        "        tuple: (final_weights, epochs, adjustments, initial_weights)\n",
        "    \"\"\"\n",
        "    if initial_weights is None:\n",
        "        rng = np.random.default_rng()\n",
        "        w = rng.uniform(-0.5, 0.5, X.shape[1])\n",
        "    else:\n",
        "        w = initial_weights.copy()\n",
        "\n",
        "    adjustments = 0\n",
        "    epoch = 0\n",
        "\n",
        "    while epoch < max_epochs:\n",
        "        error_count = 0\n",
        "        for i in range(len(X)):\n",
        "            u = np.dot(X[i], w)\n",
        "            y_hat = 1 if u >= 0 else 0\n",
        "            e = y[i] - y_hat\n",
        "            if e != 0:\n",
        "                w += eta * e * X[i]\n",
        "                adjustments += 1\n",
        "                error_count += 1\n",
        "        if error_count == 0:\n",
        "            break\n",
        "        epoch += 1\n",
        "\n",
        "    return w, epoch, adjustments, initial_weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aee5767",
      "metadata": {
        "id": "7aee5767"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Utility to load datasets provided by the professor.\n",
        "Files were saved in raw binary format using numpy.tofile().\n",
        "Each row has (x1, x2, yd).\n",
        "\"\"\"\n",
        "\n",
        "def load_dataset(path: str) -> np.ndarray:\n",
        "    \"\"\"Load dataset from binary file and reshape to (m, 3).\"\"\"\n",
        "    return np.fromfile(path, dtype=np.float64).reshape(-1, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8735cff",
      "metadata": {
        "id": "d8735cff"
      },
      "source": [
        "## Part I: Linearly Separable Problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dde8afae",
      "metadata": {
        "id": "dde8afae",
        "outputId": "8af68320-2c34-4a15-a296-8bef20bc820c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'data/dataAll.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3416603988.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/dataAll.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Features and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4043373531.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"\"\"Load dataset from binary file and reshape to (m, 3).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/dataAll.txt'"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Load the dataset dataAll.txt for Part I.\n",
        "Split into features (X) and labels (y).\n",
        "Add bias term (column of ones).\n",
        "\"\"\"\n",
        "\n",
        "# Load dataset\n",
        "data = load_dataset(\"data/dataAll.txt\")\n",
        "\n",
        "# Features and labels\n",
        "X = data[:, :2]\n",
        "y = data[:, 2].astype(int)\n",
        "\n",
        "# Add bias\n",
        "X_bias = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "print(\"--- Dataset loaded ---\")\n",
        "print(\"Shape:\", data.shape)\n",
        "print(\"Class distribution:\", np.bincount(y))\n",
        "print(\"First 5 rows:\\n\", data[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ffbc36e",
      "metadata": {
        "id": "2ffbc36e"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Split dataset into input features (X) and labels (y).\n",
        "Add bias term (column of ones) to X.\n",
        "\"\"\"\n",
        "\n",
        "# Features and labels\n",
        "X = data[:, :2]\n",
        "y = data[:, 2].astype(int)\n",
        "\n",
        "# Add bias\n",
        "X_bias = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "print(\"X_bias shape:\", X_bias.shape)\n",
        "print(\"y distribution:\", np.bincount(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4de84eba",
      "metadata": {
        "id": "4de84eba"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Train the Perceptron on the linearly separable dataset.\n",
        "Learning rate η = 0.1 (as specified in the assignment).\n",
        "\"\"\"\n",
        "\n",
        "# Generate initial weights using np.random.Generator\n",
        "rng = np.random.default_rng()\n",
        "initial_weights = rng.uniform(-0.5, 0.5, X_bias.shape[1])\n",
        "\n",
        "# Train using the same initial weights\n",
        "weights, epochs, adjustments, used_init = perceptron_train(\n",
        "    X_bias, y, eta=0.1, initial_weights=initial_weights\n",
        ")\n",
        "\n",
        "# Results\n",
        "print(\"--- Training finished ---\")\n",
        "print(f\"Initial weights: {used_init}\")\n",
        "print(f\"Final weights: {weights}\")\n",
        "print(f\"Epochs until convergence: {epochs}\")\n",
        "print(f\"Total adjustments: {adjustments}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "673b53ee",
      "metadata": {
        "id": "673b53ee"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Plot dataset points and decision boundary learned by the Perceptron.\n",
        "Class 0 = red, Class 1 = blue.\n",
        "\"\"\"\n",
        "\n",
        "# Scatter plot with class colors\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=\"RdBu\", edgecolors=\"k\", alpha=0.7)\n",
        "\n",
        "# Compute decision boundary line\n",
        "x_vals = np.linspace(min(X[:, 0]), max(X[:, 0]), 100)\n",
        "y_vals = -(weights[0] + weights[1] * x_vals) / weights[2]\n",
        "\n",
        "plt.plot(x_vals, y_vals, \"k--\", label=\"Decision boundary\")\n",
        "plt.xlabel(\"x1\")\n",
        "plt.ylabel(\"x2\")\n",
        "plt.title(\"Part I - Linearly Separable Problem\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4e6ca1b",
      "metadata": {
        "id": "b4e6ca1b"
      },
      "source": [
        "## Part II: Experimentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4a37c71",
      "metadata": {
        "id": "c4a37c71"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Experiment with multiple configurations of learning rate and\n",
        "weight initialization intervals. Perform 10 runs for each setup.\n",
        "\"\"\"\n",
        "\n",
        "# Load dataset\n",
        "data_p2 = load_dataset(\"data/data3.txt\")\n",
        "\n",
        "# Features and labels\n",
        "X_p2 = data_p2[:, :2]\n",
        "y_p2 = data_p2[:, 2].astype(int)\n",
        "\n",
        "# Add bias\n",
        "X_p2_bias = np.c_[np.ones(X_p2.shape[0]), X_p2]\n",
        "\n",
        "print(\"--- Dataset loaded ---\")\n",
        "print(\"X_bias shape:\", X_p2_bias.shape)\n",
        "print(\"y distribution:\", np.bincount(y_p2))\n",
        "print(\"\\nFirst 5 rows:\\n\", data[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97fa7f85",
      "metadata": {
        "id": "97fa7f85"
      },
      "outputs": [],
      "source": [
        "def perceptron_train_exp(X_p2, y_p2, eta=0.1, weight_range=(-0.5, 0.5)) -> tuple[np.ndarray, int, int, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Perceptron training until convergence.\n",
        "\n",
        "    Args:\n",
        "        X_p2 (ndarray): input data with bias\n",
        "        y_p2 (ndarray): labels (0 or 1)\n",
        "        eta (float): learning rate\n",
        "        max_epochs (int): maximum iterations\n",
        "        weight_range (list[float]): starting weights\n",
        "\n",
        "    Returns:\n",
        "        tuple: (final_weights, epochs, adjustments, initial_weights)\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng()\n",
        "    w: np.ndarray = rng.uniform(weight_range[0], weight_range[1], X_p2.shape[1])\n",
        "    initial_weights_print: np.ndarray = w.copy()\n",
        "\n",
        "    adjustments = 0\n",
        "    epoch = 0\n",
        "\n",
        "    # Using while until the error_cont == 0)\n",
        "    # This always will converge because the problem is linearly separable\n",
        "    while True:\n",
        "        error_count = 0\n",
        "        for i in range(len(X_p2)):\n",
        "            u = np.dot(X_p2[i], w)\n",
        "            y_hat = 1 if u >= 0 else 0\n",
        "            e = y_p2[i] - y_hat\n",
        "            if e != 0:\n",
        "                w += eta * e * X_p2[i]\n",
        "                adjustments += 1\n",
        "                error_count += 1\n",
        "\n",
        "        epoch += 1\n",
        "        if error_count == 0:\n",
        "            break\n",
        "\n",
        "    return w, epoch, adjustments, initial_weights_print"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9766620d",
      "metadata": {
        "id": "9766620d"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters to be tested\n",
        "learning_rates: list[float] = [0.4, 0.1, 0.01]\n",
        "weight_intervals: list[tuple[float, float]] = [(-100.0, 100.0), (-0.5, 0.5)]\n",
        "num_repetitions = 10\n",
        "\n",
        "# List for results\n",
        "final_results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6555da9e",
      "metadata": {
        "id": "6555da9e"
      },
      "outputs": [],
      "source": [
        "for eta in learning_rates:\n",
        "    # Loop for each weigh_intervals\n",
        "    for interval in weight_intervals:\n",
        "\n",
        "        # Temporary lists to store the 10 repeats results for each config\n",
        "        adjustments_temp = []\n",
        "        epochs_temp = []\n",
        "\n",
        "        for _ in range(num_repetitions):\n",
        "            _, epochs, adjustments, _ = perceptron_train_exp(\n",
        "                X_p2_bias, y_p2, eta=eta, weight_range=interval\n",
        "            )\n",
        "\n",
        "            adjustments_temp.append(adjustments)\n",
        "            epochs_temp.append(epochs)\n",
        "\n",
        "\n",
        "        mean_adjustments = np.mean(adjustments_temp)\n",
        "        std_adjustments = np.std(adjustments_temp)\n",
        "\n",
        "        min_epochs = min(epochs_temp)\n",
        "\n",
        "        final_results.append({\n",
        "            \"eta\": eta,\n",
        "            \"interval\": interval,\n",
        "            \"mean_adj\": mean_adjustments,\n",
        "            \"std_adj\": std_adjustments,\n",
        "            \"min_epochs\": min_epochs\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f83d8d",
      "metadata": {
        "id": "35f83d8d"
      },
      "outputs": [],
      "source": [
        "table = PrettyTable()\n",
        "table.field_names = [\n",
        "    \"Taxa de Aprendizado\",\n",
        "    \"Intervalo de Pesos\",\n",
        "    \"Quantidade de Ajustes (Média ± Desvio Padrão)\",\n",
        "    \"Menor Nº de Épocas\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eabe0c32",
      "metadata": {
        "id": "eabe0c32"
      },
      "outputs": [],
      "source": [
        "for result in final_results:\n",
        "    adj_str: str = f\"{result['mean_adj']:.2f} ± {result['std_adj']:.2f}\"\n",
        "\n",
        "    table.add_row([\n",
        "        f\"η = {result['eta']}\",\n",
        "        f\"{result['interval']}\",\n",
        "        adj_str,\n",
        "        result[\"min_epochs\"]\n",
        "    ])\n",
        "\n",
        "print(table)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76548035",
      "metadata": {
        "id": "76548035"
      },
      "source": [
        "### Discussão\n",
        "\n",
        "Intervalo de pesos é crucial: Um intervalo grande: (-100, 100) tornou o treinamento instável, apresentando um desvio padrão alto e exigindo um número muito maior de ajustes para a convergência.\n",
        "\n",
        "Taxa de aprendizado afeta a eficiência: Uma taxa de aprendizado alta (η = 0.4) precisou de menos ajustes para convergir. Enquanto que taxa muito baixa (η = 0.01) foi a mais ineficiente, precisando de muitas correções.\n",
        "\n",
        "Melhor configuração: A configuração de η = 0.4 e intervalo (-0.5, 0.5) foi a melhor, pois apresentou a combinação ideal de eficiência (menor média de ajustes) e estabilidade (menor desvio padrão), tornando o treinamento mais rápido e previsível."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a214a2ef",
      "metadata": {
        "id": "a214a2ef"
      },
      "source": [
        "## Part III: Holdout Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "863ea510-9f26-44bc-b31b-9514c4d07ecc",
      "metadata": {
        "id": "863ea510-9f26-44bc-b31b-9514c4d07ecc"
      },
      "source": [
        "#### Loading and Visualizing the Non-Linearly Separable Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "315fad37",
      "metadata": {
        "id": "315fad37"
      },
      "outputs": [],
      "source": [
        "print(\"=== Part III: Holdout Validation for Non-Linearly Separable Problem ===\")\n",
        "print(\"Loading dataset: dataHoldout.txt\")\n",
        "\n",
        "# Load the non-linearly separable dataset\n",
        "data_holdout = load_dataset(\"data/dataHoldout.txt\")\n",
        "X_holdout = data_holdout[:, :2]\n",
        "y_holdout = data_holdout[:, 2].astype(int)\n",
        "\n",
        "print(\"Dataset shape:\", data_holdout.shape)\n",
        "print(\"Class distribution:\", np.bincount(y_holdout))\n",
        "print(\"First 5 rows:\\n\", data_holdout[:5])\n",
        "\n",
        "# Visualize to show non-linear separability\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X_holdout[y_holdout == 0, 0], X_holdout[y_holdout == 0, 1],\n",
        "           c='red', alpha=0.7, label='Class 0')\n",
        "plt.scatter(X_holdout[y_holdout == 1, 0], X_holdout[y_holdout == 1, 1],\n",
        "           c='blue', alpha=0.7, label='Class 1')\n",
        "plt.title('DataHoldout Dataset - Non-Linearly Separable')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Compare with the linearly separable dataset from Part I\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X[y == 0, 0], X[y == 0, 1], c='red', alpha=0.7, label='Class 0')\n",
        "plt.scatter(X[y == 1, 0], X[y == 1, 1], c='blue', alpha=0.7, label='Class 1')\n",
        "plt.title('DataAll Dataset - Linearly Separable (Part I)')\n",
        "plt.xlabel('X1')\n",
        "plt.ylabel('X2')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Visual analysis confirms the non-linear separability of dataHoldout.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2edb051-66f5-4546-8179-005d28c81544",
      "metadata": {
        "id": "c2edb051-66f5-4546-8179-005d28c81544"
      },
      "source": [
        "#### Data Splitting (70% training / 30% testing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df8df1e0-8d85-4d27-8efc-978b21f039e4",
      "metadata": {
        "id": "df8df1e0-8d85-4d27-8efc-978b21f039e4"
      },
      "outputs": [],
      "source": [
        "print(\"=== Data Splitting ===\")\n",
        "\n",
        "# Set random seed based on team identifier\n",
        "team_identifier = 3  # From cell 4 in the notebook\n",
        "random_seed = team_identifier\n",
        "print(f\"Using random seed: {random_seed}\")\n",
        "\n",
        "# Add bias term\n",
        "X_holdout_bias = np.c_[np.ones(X_holdout.shape[0]), X_holdout]\n",
        "\n",
        "# Split data into training and testing sets\n",
        "np.random.seed(random_seed)\n",
        "indices = np.random.permutation(len(X_holdout_bias))\n",
        "split_point = int(0.7 * len(X_holdout_bias))\n",
        "\n",
        "train_indices = indices[:split_point]\n",
        "test_indices = indices[split_point:]\n",
        "\n",
        "X_train = X_holdout_bias[train_indices]\n",
        "y_train = y_holdout[train_indices]\n",
        "X_test = X_holdout_bias[test_indices]\n",
        "y_test = y_holdout[test_indices]\n",
        "\n",
        "print(f\"Training set size: {len(X_train)} samples ({len(X_train)/len(X_holdout_bias)*100:.1f}%)\")\n",
        "print(f\"Test set size: {len(X_test)} samples ({len(X_test)/len(X_holdout_bias)*100:.1f}%)\")\n",
        "print(f\"Training class distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test class distribution: {np.bincount(y_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc624141-1239-4df0-b4ec-1f1b0984beb5",
      "metadata": {
        "id": "dc624141-1239-4df0-b4ec-1f1b0984beb5"
      },
      "source": [
        "#### Modified Perceptron Training Function for 100 Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afed217e-8654-4964-b62e-3210209f44c7",
      "metadata": {
        "id": "afed217e-8654-4964-b62e-3210209f44c7"
      },
      "outputs": [],
      "source": [
        "# Cell 3: Modified Perceptron Training Function for 100 Epochs\n",
        "print(\"=== Perceptron Training Configuration ===\")\n",
        "\n",
        "def perceptron_train_holdout(X, y, eta=0.1, max_epochs=100, initial_weights=None, random_seed=3):\n",
        "    \"\"\"\n",
        "    Perceptron training with random presentation order each epoch.\n",
        "\n",
        "    Args:\n",
        "        X: input data with bias\n",
        "        y: labels (0 or 1)\n",
        "        eta: learning rate\n",
        "        max_epochs: maximum iterations\n",
        "        initial_weights: optional starting weights\n",
        "        random_seed: seed for random ordering\n",
        "\n",
        "    Returns:\n",
        "        tuple: (final_weights, training_history, all_weights)\n",
        "    \"\"\"\n",
        "    if initial_weights is None:\n",
        "        rng = np.random.default_rng(random_seed)\n",
        "        w = rng.uniform(-0.5, 0.5, X.shape[1])\n",
        "    else:\n",
        "        w = initial_weights.copy()\n",
        "\n",
        "    training_history = {\n",
        "        'epoch_errors': [],\n",
        "        'weights_history': [],\n",
        "        'adjustments_per_epoch': []\n",
        "    }\n",
        "\n",
        "    epoch = 0\n",
        "    all_weights = [w.copy()]\n",
        "\n",
        "    while epoch < max_epochs:\n",
        "        # Randomize presentation order each epoch\n",
        "        np.random.seed(random_seed + epoch)  # Different seed each epoch\n",
        "        order = np.random.permutation(len(X))\n",
        "\n",
        "        error_count = 0\n",
        "        adjustments = 0\n",
        "\n",
        "        for i in order:\n",
        "            u = np.dot(X[i], w)\n",
        "            y_hat = activation_function(u)\n",
        "            e = y[i] - y_hat\n",
        "\n",
        "            if e != 0:\n",
        "                w += eta * e * X[i]\n",
        "                adjustments += 1\n",
        "                error_count += 1\n",
        "\n",
        "        training_history['epoch_errors'].append(error_count)\n",
        "        training_history['adjustments_per_epoch'].append(adjustments)\n",
        "        training_history['weights_history'].append(w.copy())\n",
        "        all_weights.append(w.copy())\n",
        "\n",
        "        epoch += 1\n",
        "\n",
        "    return w, training_history, all_weights\n",
        "\n",
        "print(\"Training configuration:\")\n",
        "print(f\"- Learning rate (η): 0.1\")\n",
        "print(f\"- Max epochs: 100\")\n",
        "print(f\"- Weight initialization: U(-0.5, +0.5)\")\n",
        "print(f\"- Activation: step function (θ=0)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ccda3bf-5cdc-4604-8aec-32ed52a6ae45",
      "metadata": {
        "id": "7ccda3bf-5cdc-4604-8aec-32ed52a6ae45"
      },
      "source": [
        "#### Training the Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3480d455-b615-43d5-b170-625e47aea4f4",
      "metadata": {
        "id": "3480d455-b615-43d5-b170-625e47aea4f4"
      },
      "outputs": [],
      "source": [
        "print(\"=== Training Perceptron for 100 Epochs ===\")\n",
        "\n",
        "# Train the perceptron\n",
        "final_weights, training_history, all_weights = perceptron_train_holdout(\n",
        "    X_train, y_train, eta=0.1, max_epochs=100, random_seed=random_seed\n",
        ")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "print(f\"Final weights: {final_weights}\")\n",
        "\n",
        "# Plot training progress\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(training_history['epoch_errors'])\n",
        "plt.title('Errors per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Number of Errors')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(training_history['adjustments_per_epoch'])\n",
        "plt.title('Weight Adjustments per Epoch')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Number of Adjustments')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "# Plot weight evolution\n",
        "weights_array = np.array(all_weights)\n",
        "for i in range(weights_array.shape[1]):\n",
        "    plt.plot(weights_array[:, i], label=f'w{i}')\n",
        "plt.title('Weight Evolution During Training')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Weight Value')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Note: The persistent errors indicate non-linear separability\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef2775da-3b84-4661-9e88-013dd599e871",
      "metadata": {
        "id": "ef2775da-3b84-4661-9e88-013dd599e871"
      },
      "source": [
        "#### Predictions and Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "508ee52f-dce1-48c7-a318-db77f111b4b7",
      "metadata": {
        "id": "508ee52f-dce1-48c7-a318-db77f111b4b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "5211b5b4-6797-4331-932b-65f65ba24de1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Model Evaluation on Test Set ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'final_weights' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2760453494.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Make predictions on test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0my_test_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_perceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Calculate metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_weights' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"=== Model Evaluation on Test Set ===\")\n",
        "\n",
        "def predict_perceptron(X, weights):\n",
        "    \"\"\"Make predictions using trained perceptron weights\"\"\"\n",
        "    predictions = []\n",
        "    for sample in X:\n",
        "        u = np.dot(sample, weights)\n",
        "        predictions.append(activation_function(u))\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Make predictions on test set\n",
        "y_test_pred = predict_perceptron(X_test, final_weights)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(y_test, y_test_pred)\n",
        "precision = precision_score(y_test, y_test_pred, zero_division=0)\n",
        "recall = recall_score(y_test, y_test_pred, zero_division=0)\n",
        "f1 = f1_score(y_test, y_test_pred, zero_division=0)\n",
        "conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
        "\n",
        "print(\"1. CONFUSION MATRIX:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Plot confusion matrix as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix - Perceptron on Test Set')\n",
        "plt.colorbar()\n",
        "\n",
        "classes = ['Class 0', 'Class 1']\n",
        "tick_marks = np.arange(len(classes))\n",
        "plt.xticks(tick_marks, classes)\n",
        "plt.yticks(tick_marks, classes)\n",
        "\n",
        "# Add text annotations\n",
        "thresh = conf_matrix.max() / 2.\n",
        "for i, j in np.ndindex(conf_matrix.shape):\n",
        "    plt.text(j, i, format(conf_matrix[i, j], 'd'),\n",
        "             horizontalalignment=\"center\",\n",
        "             color=\"white\" if conf_matrix[i, j] > thresh else \"black\")\n",
        "\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n2. ACCURACY:\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "# Also calculate training accuracy for comparison\n",
        "y_train_pred = predict_perceptron(X_train, final_weights)\n",
        "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def predict_linear(X, w):\n",
        "    return (X @ w >= 0).astype(int)\n",
        "\n",
        "y_pred = predict_linear(X_test, final_weights)\n",
        "\n",
        "print(\"Precisão:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F-Score:\", f1_score(y_test, y_pred))\n",
        "\n",
        "def plot_decision_boundary_w(X, y, w, title):\n",
        "    x_min, x_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    y_min, y_max = X[:, 2].min() - 1, X[:, 2].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                         np.arange(y_min, y_max, 0.1))\n",
        "    grid = np.c_[np.ones(xx.size), xx.ravel(), yy.ravel()]\n",
        "    Z = predict_linear(grid, w).reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X[:, 1], X[:, 2], c=y, edgecolor='k')\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_decision_boundary_w(X_train, y_train, final_weights, \"Training data with decision boundary\")\n",
        "plot_decision_boundary_w(X_test, y_test, final_weights, \"Test data with decision boundary\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "FdKA9NuPHfb8",
        "outputId": "a4cbdf53-7a3a-437d-9f9c-b7b770937b36"
      },
      "id": "FdKA9NuPHfb8",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'final_weights' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2809386480.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precisão:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'final_weights' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f055497",
      "metadata": {
        "id": "5f055497"
      },
      "source": [
        "## Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f3d81b82",
      "metadata": {
        "id": "f3d81b82"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}